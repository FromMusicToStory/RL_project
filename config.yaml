# model
model_name: "klue/roberta-base"
batch_size: 32
num_workers: 4

# trainer
ngpu: 1
max_epochs: 200
check_val_every_n_epoch: 2
accumulate_grad_batches: 1
log_every_n_steps: 100
checkpoint_path: "checkpoint/"

# optimizer
lr: 5e-5

# hyperparameter for RL
gamma: 0.9
sync_rate: 10

# eps
eps_start: 1.0
eps_end: 0.01

# populate
warm_start_steps: 1000
eps: 1.0

# wandb logger
project: RL
name: DQN_base